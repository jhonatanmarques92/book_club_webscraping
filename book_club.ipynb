{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-23T00:37:59.794758Z",
     "start_time": "2022-04-23T00:37:57.049008Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-23T00:37:59.841761Z",
     "start_time": "2022-04-23T00:37:59.817762Z"
    }
   },
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-23T00:37:59.903760Z",
     "start_time": "2022-04-23T00:37:59.857761Z"
    }
   },
   "outputs": [],
   "source": [
    "def url_bs(url):\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = bs(page.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Coleta de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-23T00:40:53.029987Z",
     "start_time": "2022-04-23T00:37:59.915762Z"
    }
   },
   "outputs": [],
   "source": [
    "# lista de categorias\n",
    "lista_categoria = []\n",
    "soup = url_bs('https://books.toscrape.com/')\n",
    "aux = soup.find('ul', class_='nav nav-list').find_all('a')\n",
    "aux1 = [p.get_text('href') for p in aux]\n",
    "for e in aux1:\n",
    "    aux2 = re.findall('\\w{0,}.\\S+', e)\n",
    "    aux2 = ''.join(aux2)\n",
    "    lista_categoria.append(aux2)\n",
    "\n",
    "del lista_categoria[0]\n",
    "\n",
    "# lista das url das categorias\n",
    "browser = webdriver.Chrome(chrome_options=chrome_options)\n",
    "browser.get('https://books.toscrape.com')\n",
    "lista_url_categoria = []\n",
    "for i in range(1, len(lista_categoria) + 1):\n",
    "    browser.find_element_by_xpath(f'//*[@id=\"default\"]/div[1]/div/div/aside/div[2]/ul/li/ul/li[{i}]/a').click()\n",
    "    lista_url_categoria.append(browser.current_url)\n",
    "    \n",
    "# Webscraping\n",
    "\n",
    "# Listas auxiliares\n",
    "lista_nome = []\n",
    "lista_preco = []\n",
    "lista_avaliacao = []\n",
    "lista_estoque = []\n",
    "lista_categoria_2 = []\n",
    "\n",
    "for i in range (len(lista_categoria)):\n",
    "    categoria = lista_categoria[i]\n",
    "    url_categoria = lista_url_categoria[i]\n",
    "    browser.get(url_categoria)\n",
    "    soup = url_bs(url_categoria)   \n",
    "    \n",
    "################  PÁGINA 1 ####################  \n",
    "################################### NOME DO LIVRO ##############################################\n",
    "    aux_nome = soup.find_all('h3')\n",
    "    nome = [p.get_text() for p in aux_nome]\n",
    "    lista_nome = lista_nome + nome\n",
    "    \n",
    "#################################### CATEGORIA #################################################\n",
    "    aux_categoria = soup.find('form', class_='form-horizontal').find_all('strong')\n",
    "    aux_categoria_2 = [p.get_text() for p in aux_categoria][0]\n",
    "    for e in range(int(aux_categoria_2)):\n",
    "        lista_categoria_2.append(categoria)\n",
    "    \n",
    "################################## PREÇO DO LIVRO ##############################################\n",
    "    aux_preco = soup.find_all('p', class_='price_color')\n",
    "    preco = [p.get_text() for p in aux_preco]\n",
    "    lista_preco = lista_preco + preco\n",
    "\n",
    "################################ AVALIAÇÃO DO LIVRO ############################################\n",
    "    aux_avaliacao = soup.find_all('p', class_='star-rating')\n",
    "    avaliacao = [p.get('class')[1] for p in aux_avaliacao]\n",
    "    lista_avaliacao = lista_avaliacao + avaliacao\n",
    "    \n",
    "#################################### ESTOQUE ###################################################\n",
    "    aux_estoque = soup.find_all('p', class_='instock availability')\n",
    "    estoque = [p.get_text() for p in aux_estoque]\n",
    "    lista_estoque = lista_estoque + estoque\n",
    "    \n",
    "    # Verificar se há paginação\n",
    "    paginacao = soup.find('li', class_='current')\n",
    "    if paginacao:\n",
    "        aux = soup.find('li', class_='current').get_text()\n",
    "        aux1 = re.findall('\\d{0,}', aux)\n",
    "        aux2 = list(filter(None, aux1))[1]\n",
    "        aux2 = int(aux2) - 1\n",
    "\n",
    "        for p in range(aux2 + 1):\n",
    "            if p == 0: \n",
    "               ######### ENVIA PARA A PÁGINA 2 ##################  \n",
    "                browser.find_element_by_xpath('//*[@id=\"default\"]/div[1]/div/div/div/section/div[2]/div/ul/li[2]/a').click()\n",
    "                soup = url_bs(browser.current_url)\n",
    "\n",
    "            elif (p > 0) & (p != aux2): ######## PÁGINAS 2 ###############                \n",
    "################################### NOME DO LIVRO ##############################################\n",
    "                aux_nome = soup.find_all('h3')\n",
    "                nome = [p.get_text() for p in aux_nome]\n",
    "                lista_nome = lista_nome + nome\n",
    "            \n",
    "################################## PREÇO DO LIVRO ##############################################\n",
    "                aux_preco = soup.find_all('p', class_='price_color')\n",
    "                preco = [p.get_text() for p in aux_preco]\n",
    "                lista_preco = lista_preco + preco\n",
    "            \n",
    "################################ AVALIAÇÃO DO LIVRO ############################################\n",
    "                aux_avaliacao = soup.find_all('p', class_='star-rating')\n",
    "                avaliacao = [p.get('class')[1] for p in aux_avaliacao]\n",
    "                lista_avaliacao = lista_avaliacao + avaliacao\n",
    "            \n",
    "#################################### ESTOQUE ###################################################\n",
    "                aux_estoque = soup.find_all('p', class_='instock availability')\n",
    "                estoque = [p.get_text() for p in aux_estoque]\n",
    "                lista_estoque = lista_estoque + estoque\n",
    "            \n",
    "                ######### PRÓXIMA PÁGINA #######\n",
    "                browser.find_element_by_xpath('//*[@id=\"default\"]/div[1]/div/div/div/section/div[2]/div/ul/li[3]/a').click()\n",
    "                soup = url_bs(browser.current_url)\n",
    "\n",
    "            else: ######## PÁGINA 3 EM DIANTE ###########               \n",
    "################################### NOME DO LIVRO ##############################################\n",
    "                aux_nome = soup.find_all('h3')\n",
    "                nome = [p.get_text() for p in aux_nome]\n",
    "                lista_nome = lista_nome + nome \n",
    "            \n",
    "################################## PREÇO DO LIVRO ##############################################\n",
    "                aux_preco = soup.find_all('p', class_='price_color')\n",
    "                preco = [p.get_text() for p in aux_preco]\n",
    "                lista_preco = lista_preco + preco\n",
    "\n",
    "################################ AVALIAÇÃO DO LIVRO ############################################\n",
    "                aux_avaliacao = soup.find_all('p', class_='star-rating')\n",
    "                avaliacao = [p.get('class')[1] for p in aux_avaliacao]\n",
    "                lista_avaliacao = lista_avaliacao + avaliacao\n",
    "    \n",
    "#################################### ESTOQUE ###################################################\n",
    "                aux_estoque = soup.find_all('p', class_='instock availability')\n",
    "                estoque = [p.get_text() for p in aux_estoque]\n",
    "                lista_estoque = lista_estoque + estoque\n",
    "\n",
    " \n",
    "browser.close()\n",
    "\n",
    "data = {'nome':lista_nome, 'categoria':lista_categoria_2, 'preco':lista_preco, 'avaliacao':lista_avaliacao, 'estoque':lista_estoque}\n",
    "df = pd.DataFrame(data)\n",
    "df['data'] = datetime.now().strftime('%Y-%m-%d')\n",
    "df['horario'] = datetime.now().strftime('%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-23T00:40:53.109980Z",
     "start_time": "2022-04-23T00:40:53.036982Z"
    }
   },
   "outputs": [],
   "source": [
    "df['estoque'] = df['estoque'].str.extract('(In stock)')\n",
    "df['preco'] = df['preco'].str.extract('(\\d.+)')\n",
    "\n",
    "df.loc[df['avaliacao'] == 'One', 'avaliacao'] = 1\n",
    "df.loc[df['avaliacao'] == 'Two', 'avaliacao'] = 2\n",
    "df.loc[df['avaliacao'] == 'Three', 'avaliacao'] = 3\n",
    "df.loc[df['avaliacao'] == 'Four', 'avaliacao'] = 4\n",
    "df.loc[df['avaliacao'] == 'Five', 'avaliacao'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Exportando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-23T00:40:53.156987Z",
     "start_time": "2022-04-23T00:40:53.120979Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('teste.csv', encoding='latin-1', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
